{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1i9HeIb4E0S"
      },
      "source": [
        "# Step1 : Install yt-dlp and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wkrgdgA4E0T"
      },
      "outputs": [],
      "source": [
        "!pip install yt-dlp google-cloud-aiplatform webvtt-py\n",
        "\n",
        "import os\n",
        "import yt_dlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClWh1pTN4E0U"
      },
      "source": [
        "# Step 2: Define a function to download audio and transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6P33shPh4E0U",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726510274827,
          "user_tz": 420,
          "elapsed": 182,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "def download_audio_and_transcript(youtube_link):\n",
        "    opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '128',\n",
        "        }],\n",
        "        'writesubtitles': True,\n",
        "        'writeautomaticsub': True,\n",
        "        'subtitlesformat': 'txt',\n",
        "        'skip_download': False\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(opts) as ydl:\n",
        "        info_dict = ydl.extract_info(youtube_link, download=True)\n",
        "        audio_file = f\"{info_dict['id']}.mp3\"\n",
        "\n",
        "        subtitles = info_dict.get('subtitles')\n",
        "        transcript_file = None\n",
        "        if subtitles:\n",
        "            if 'en' in subtitles:\n",
        "                transcript_file = f\"{info_dict['id']}_en.txt\"\n",
        "            else:\n",
        "                first_lang = list(subtitles.keys())[0]\n",
        "                transcript_file = f\"{info_dict['id']}_{first_lang}.txt\"\n",
        "        return audio_file, transcript_file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j3-FO7sf4E0U",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726510275658,
          "user_tz": 420,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "youtube_link = \"https://www.youtube.com/watch?v=3A855rN_9pE&t=162s\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftJZJVa94E0V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726510340474,
          "user_tz": 420,
          "elapsed": 64602,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "eee324d9-ee47-4903-feb0-c045f01bf221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=3A855rN_9pE&t=162s\n",
            "[youtube] 3A855rN_9pE: Downloading webpage\n",
            "[youtube] 3A855rN_9pE: Downloading ios player API JSON\n",
            "[youtube] 3A855rN_9pE: Downloading web creator player API JSON\n",
            "[youtube] 3A855rN_9pE: Downloading m3u8 information\n",
            "[info] 3A855rN_9pE: Downloading subtitles: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: No subtitle format found matching \"txt\" for language en, using vtt. Use --list-subs for a list of available subtitles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] 3A855rN_9pE: Downloading 1 format(s): 251\n",
            "Deleting existing file 3A855rN_9pE.en.vtt\n",
            "[info] Writing video subtitles to: 3A855rN_9pE.en.vtt\n",
            "[download] Destination: 3A855rN_9pE.en.vtt\n",
            "[download] 100% of  333.25KiB in 00:00:00 at 2.42MiB/s\n",
            "[download] 3A855rN_9pE.webm has already been downloaded\n",
            "[download] 100% of   36.70MiB\n",
            "[ExtractAudio] Destination: 3A855rN_9pE.mp3\n",
            "Deleting original file 3A855rN_9pE.webm (pass -k to keep)\n",
            "Audio downloaded: 3A855rN_9pE.mp3\n",
            "No transcript found for this video.\n"
          ]
        }
      ],
      "source": [
        "audio_file, transcript_file = download_audio_and_transcript(youtube_link)\n",
        "\n",
        "if audio_file:\n",
        "    print(f\"Audio downloaded: {audio_file}\")\n",
        "else:\n",
        "    print(\"Audio download failed\")\n",
        "\n",
        "if transcript_file:\n",
        "    print(f\"Transcript downloaded: {transcript_file}\")\n",
        "else:\n",
        "    print(\"No transcript found for this video.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yqFGmgW4E0V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726512053032,
          "user_tz": 420,
          "elapsed": 247995,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "51739f69-88ce-4dea-a315-f6b4059d8494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "00:00\n",
            "Good morning, everyone. Thank you so much for coming to this breakout session on data governance in the age of AI.\n",
            "\n",
            "We're very excited to have with us two customer speakers today. \n",
            "We have Cynthia Gums, who is the manager of global data insights and analytics at Ford, driving key initiatives around the new data factory at Ford. \n",
            "We also have Steve Jarrett, who is the chief AI officer at Orange, leading AI and data strategy for Orange across 26 countries. \n",
            "And my name is Lou Ann, I'm a product manager for Dataplex, here at Google Cloud. \n",
            "\n",
            "So, we're very excited to be sharing with you how we think about data governance in this age of AI and what do our journeys each look like.\n",
            "\n",
            "So, here's our agenda for today. We're gonna start with an introduction and a product overview followed by case studies from Ford and Orange.\n",
            "And then we'll talk about what's new, what's upcoming in Dataplex.\n",
            "\n",
            "So, as all of us have experienced recently, generative AI is really this paradigm shift that is revolutionizing how we operate as businesses.\n",
            "Whether it is generating creative content, whether it is working with complex data, whether it's improving your customer experiences, or even training your own large language models for enterprise use cases. \n",
            "The impact of AI is really profound and all-encompassing.\n",
            "\n",
            "05:00\n",
            "At the same time, we know that data is the fuel that feeds into the engine of AI. \n",
            "It is really the critical foundation for training and grounding your models.\n",
            "And in return, this rapid growth in AI innovation is really creating an accelerating demand for data that is well-governed, high quality and easy to discover.\n",
            "\n",
            "So, given this strong need and North Star vision, what are the challenges that companies are actually facing in reality?\n",
            "\n",
            "Well, first and foremost, we have the challenge of dark data, which I'm sure most of you could resonate with.\n",
            "In fact, we know that 66% of organizations have reported that at least half of their data is dark, which means that it is data that is not even discovered or used in the first place.\n",
            "\n",
            "And even if you're able to discover and use that data, there's still a lot of questions about data quality. \n",
            "Whether this data is valid, whether this data is trustworthy.\n",
            "\n",
            "We learn from our survey that only 44% of data leaders are fully confident in the quality of their organization's data.\n",
            "And as we all know, low quality data would only result in low quality outputs which you really cannot trust for any insight generation or decision making.\n",
            "\n",
            "Now, the reason why managing and governing your data is so difficult is due to the complexity of the enterprise data landscape.\n",
            "\n",
            "As you can see here on the diagram, data is really coming in from various different sources. \n",
            "There's stored and processed in different services. \n",
            "Whether it's data warehouse, data lakes, or data bases, they reside in different formats, and they're used by different personas across different workflows.\n",
            "\n",
            "10:00\n",
            "Now, please raise your hand if this complex situation ever seemed familiar to you.\n",
            "\n",
            "Yes, I see a lot of hands raised. Definitely, that's what we see all the time as well.\n",
            "So, this challenge is exactly what's keeping us busy here at Google Cloud.\n",
            "And as you may have learned from this conference so far, we're really evolving BigQuery into a unified data and AI governance platform. \n",
            "A unified data and AI platform and this platform is designed with data governance as a central building consideration, that is contextual and pervasive across the different layers of the tech stack.\n",
            "\n",
            "Now, at the heart of providing this unified data and AI governance is Dataplex, which is our native offering for automating data governance and data management at scale. \n",
            "There are several key value propositions of Dataplex we have seen truly resonating with our customers based on interactions.\n",
            "\n",
            "First and foremost, Dataplex deeply integrates with various products and services to really provide this unified metadata across distributed data.\n",
            "And based on this, you're able to perform search across different projects, across different regions, and across different data silos.\n",
            "\n",
            "And based on that, you're also able to further enrich and organize your data as needed. \n",
            "So, that's number one.\n",
            "\n",
            "Number two, on top of this wealth of metadata, Dataplex offers centralized security and governance features.\n",
            "This really allows you to easily manage your data governance policies based on understanding of the metadata context.\n",
            "\n",
            "15:00\n",
            "And last but not least, Dataplex has a rich set of features around intelligent data management, from tracking data lineage to assessing data profile, and to automating data quality checks.\n",
            "So, really helping you build better trust in your data, and helping you optimize data related ROI.\n",
            "\n",
            "Now, since GA launch back in 2022, Dataplex has been widely adopted by customers across different geographies and different industry verticals.\n",
            "As of now, over 95% of the top data analytics customers at Google Cloud are already using Dataplex for managing and governing their data at scale.\n",
            "\n",
            "So today, we're very excited to be hearing from two of them, Ford and Orange.\n",
            "So, please join me in first welcoming Cynthia from Ford to talk about her journey with data governance.\n",
            "\n",
            "Good day! How's everybody doing today?\n",
            "\n",
            "So, my name is Cynthia Gums and I'm responsible for data discovery and classification at Ford Motor Company. \n",
            "Welcome to my TED Talk. \n",
            "\n",
            "No, I'm just kidding.\n",
            "\n",
            "So, while this may not be a TED Talk, I promise you that this is an important topic and I'm really honored to have the opportunity to speak with you today about it. \n",
            "Okay?\n",
            "\n",
            "So, before we start, ooh, that's loud! Before we start, I'm curious about who's in the crowd today.\n",
            "So, how many of you consider yourselves a data governance professional? You can just wave your hand.\n",
            "Alright, there's a lot of you out there. And if you didn't raise your hand, how many of you have an appreciation for what data governance is?\n",
            "\n",
            "Okay, so most of you, awesome!\n",
            "\n",
            "So I, as IT and data professionals, I'm sure you've had the opportunity to explain to non-data or non-tech people what you do for a living, right?\n",
            "\n",
            "So, when you tell someone, \"I work in IT, information technology.\" They look at you and they think, \"Oh, you must be doing tech support\" and then they start calling you to help them with their Wi-Fi, right?\n",
            "\n",
            "Then you tell them, \"Well, I work in the data analytics department.\" And then they think, \"Oh, she just creates charts and things all day, she doesn't really do a whole lot of anything cuz that's what data analytics is\", right?\n",
            "\n",
            "Then you tell them, \"Well, I'm in data governance.\" And now they think you have the magic key to give everybody access to data. \n",
            "Data governance is so much more than that.\n",
            "And then for me, when I say I work in data discovery and classification, they have no clue what I'm talking about.\n",
            "And so then I have to explain to them, \"I am responsible for making sure people can find the data they need to solve business problems.\"\n",
            "\n",
            "And then I might give an analogy about a shopping experience or being in a library and then they get it, okay?\n",
            "So, data governance is a broad topic. \n",
            "Today, I'm just gonna scratch the surface of it but I'm really gonna dig a little bit deeper into data discovery. \n",
            "\n",
            "So, our Ford data platform is powered by Google Cloud.\n",
            "And we believe that data is awesome, valuable and worthy of respect. \n",
            "\n",
            "20:00\n",
            "Our platform objective is to establish a single source of truth of data, to enable data fusion and responsible data usage.\n",
            "\n",
            "Our platform helps data engineers organize many, many data sources across every aspect of our business. \n",
            "We have a very complex environment and we have a number of key capabilities that allow us to meet our objective. \n",
            "Data governance is a foundational capability for our platform.\n",
            "\n",
            "As a part of our migration to Google Cloud, we have enabled Dataplex data catalog to capture technical and business metadata about our projects and data sets.\n",
            "\n",
            "Each of our governance teams benefits from Dataplex capabilities, the main one is tag templates for the collection of business metadata. \n",
            "We also benefit from Dataplex APIs to help expose that metadata in our custom user interfaces, as well as our backend processes.\n",
            "\n",
            "Additionally, our data quality team recently launched data lineage. \n",
            "And data lineage allows us to understand where the data came from and its lifecycle. \n",
            "We also use lineage to understand how to troubleshoot the data, how to identify dependencies and also for data discovery.\n",
            "\n",
            "So, as I previously, previously mentioned, we use tag templates as the foundation for our data catalog experience.\n",
            "\n",
            "And this metadata is collected as a part of our end-to-end data process. \n",
            "So, when you onboard data, when you create your project, all the way through access enablement, we're collecting the data along the way.\n",
            "\n",
            "We collect the metadata at every level of our project hierarchy, starting with projects, then data sets, tables, views, and columns. \n",
            "We have tag templates for each of those levels.\n",
            "\n",
            "Today, we're collecting roughly 70 business metadata tags. \n",
            "That might not sound like a lot, but it's, it's important data that we need to help people discover the data. \n",
            "Actually, Dataplex lets you capture thousands of tags. \n",
            "\n",
            "25:00\n",
            "Most of this metadata is captured manually. \n",
            "So, you can imagine, it's kind of tedious, time-consuming and resource-intensive.\n",
            "\n",
            "But we're working with um, we're experimenting with Gen AI to see if we can do some automation in the metadata enrichment space. \n",
            "\n",
            "We have custom user interfaces to allow our users to input and extract the metadata from the data catalog.\n",
            "\n",
            "Now, you might be wondering, \"Well, why do you need a custom user interface? Dataplex has an interface via the console.\"\n",
            "\n",
            "However, there's many reasons why we decided to go this route, but the main one is we need to control the way that metadata is input and the way it is exposed to our users. \n",
            "Right now, today in Dataplex, which is perfect for a platform team, it shows you everything: staging tables, temp tables. \n",
            "That's great for the platform team, but your end users don't want to see that. \n",
            "And so, we try to modify the experience so that we can curate it for excellence.\n",
            "\n",
            "Okay.\n",
            "\n",
            "Another reason why we created a custom user experience is because we have so many different personas that we're trying to satisfy. \n",
            "So, last year, we launched our custom data discovery experience. \n",
            "This was an exciting time for us because we've been on quite a journey trying to solve the data discovery challenge.\n",
            "\n",
            "This experience makes use of the Dataplex APIs and we call it our Data Discovery Hub.\n",
            "This interface currently have over 9,000 users and growing. \n",
            "\n",
            "Our users are able to search the data catalog, and when the results come back, they're also able to see all the metadata related to that data asset. \n",
            "They can also see the table and view schemas as well. \n",
            "We've enabled an export capability that allows users to export their search results, the table schemas, and the metadata if they need to do any offline analysis.\n",
            "Now, I need to be clear, this is not an export of the data. \n",
            "This is export of the metadata, right? The security folks are probably looking for me now. \n",
            "\n",
            "30:00\n",
            "And so, our user interface also creates, or provides, linkages to other tools that we have. \n",
            "We have our data activation portal which allows users to request access to the data that they've discovered.\n",
            "Then, we also have our data quality dashboards so they can see the completeness, the timeliness, and the other measures for data quality. \n",
            "\n",
            "Alright. \n",
            "\n",
            "Alright.\n",
            "\n",
            "So, we are currently, uh-oh. \n",
            "Sorry, guys. \n",
            "Alright, so one of the key challenges for our data discovery activity is that, um, the user experience is really important. \n",
            "And our challenge was that we have all these different user personas.\n",
            "\n",
            "We have data analysts, data stewards, data scientists, software analysts, data engineers, and then you have your business users. \n",
            "All these people have a need to discover data, but their needs are a little bit different. \n",
            "And so, if I return to a business user this big long list of tables and columns, they're not going to know what to do with that, right?\n",
            "\n",
            "And so, to solve this problem, we've engaged project, project designers to help us go through this full effort of understanding our personas. \n",
            "What do they want?\n",
            "What are they thinking about when they search for data?\n",
            "How could we return the results such that they have an appreciation for what those, um, what the data is and what it can do for them?\n",
            "\n",
            "35:00\n",
            "We've done user interviews, sessions where we sit down with them and watch them use the tool, we've done surveys, and all of that has resulted in excellent feedback that we're using to inform how we modify our product. \n",
            "\n",
            "Another challenge is ensuring that we have high quality, relevant metadata.\n",
            "If your columns don't have descriptions, how can someone search and find that your column has the data that they need to solve their business problem?\n",
            "\n",
            "However, I already mentioned that populating metadata is time-consuming and resource-intensive.\n",
            "And so, how do you solve that problem? \n",
            "One thing that we've done is we created a tool that allows data teams to update their descriptions in bulk using their data dictionaries as input. \n",
            "And so now, they already have a data dictionary. \n",
            "They can upload that, it goes through Terraform and it updates their descriptions behind the scenes. \n",
            "So, that was a great enhancement that we enabled.\n",
            "\n",
            "So, what's next for data discovery at Ford?\n",
            "\n",
            "We are currently piloting a data marketplace experience which allows users to search, find and access data, well request access to data, in the same user experience. \n",
            "Today, it's separate experiences, but we're merging them, right?\n",
            "And so, this was a highly requested item from our users that we're trying to satisfy.\n",
            "So far, the pilot is going really well and we're getting excellent feedback that will inform our, our product.\n",
            "\n",
            "And now, we're also looking then to launch some additional enhancements to the experience. \n",
            "So, previously, it was just a simple search experience, but, but now it's search that will have best bets enabled, that's what we're calling it, best bets. \n",
            "\n",
            "40:00\n",
            "Best bets means um, it's a keyword focused activity where we have popular data sets, keywords that describe them. \n",
            "When users go in to do their search if they hit one of those keywords, those items will bubble to the top of the search. \n",
            "\n",
            "We're also allowing our users to add comments and ratings for the data. \n",
            "So, if you already have access to the data, you can find it in the catalog and say, \"This data set was awesome, it helped me do X, Y or Z. Try it out\", right? And if they thumbs it up, that also influences where it shows up in the search results. \n",
            "\n",
            "The next one is around data collections. \n",
            "So, it's kind of like a storefront for the data. \n",
            "At Ford, we have a number of different subject areas and they want to see their data together, right? \n",
            "They don't want to see it mixed up with everybody else's data. \n",
            "And so, we're looking to create these collections or storefronts, so those different subject areas can say, \"You want to use my data, go to this place in the data catalog to find it.\" \n",
            "Okay?\n",
            "\n",
            "So, we're also experimenting with Gen AI. \n",
            "I'm sure you've been hearing a lot about Gen AI during the conference this week. \n",
            "And so, we're looking at using Gen AI to assist with metadata enrichment, automating it, as well as helping to generate business descriptions for the data. \n",
            "I already mentioned it's time in, you know, time-consuming to do so manually. \n",
            "But how cool would it be if you could just throw Gen AI at a table and say, \"What's in this table?\" and it figures it out because it can tell what's in the table from the data and it will describe it and you move on. \n",
            "Now, you might have to verify that the descriptions are decent. \n",
            "But once you get that confidence, I think it'll be a powerful data discovery experience. \n",
            "\n",
            "45:00\n",
            "Alright, and then we're also looking at doing an LLM, we're training it on documentation about the data as well as the data catalog itself.\n",
            "Now, you're taking data discovery to another level, right?\n",
            "Because you can do natural language queries, ask it questions, and it's gonna combine that documentation with that metadata and give you a really good answer about what data is available to solve your business problems. \n",
            "\n",
            "Lastly, we're really looking forward to implementing Dataplex's business terms in glossary. \n",
            "This will allow us to give common understanding to those key terms that are the same across the company. \n",
            "So, if you have 50 different data sources and they all mention part number, do we have to define part number 50 times in 50 different ways? No.\n",
            "But the busi- business terms will allow us to have that be consistent across all the data. \n",
            "\n",
            "So, we've been on this journey for a little over two years, and it's been challenging but rewarding at the same time, especially with this data marketplace launch.\n",
            "And so, I want to show appreciation to Google, my team, and also all the teams involved for their engagement and collaboration, because it's, it's been fun.\n",
            "I was, um, in another session and the gentleman said data governance was boring.\n",
            "\n",
            "Really?\n",
            "\n",
            "We're having so much fun trying to figure out how to solve this Dataplex problem, well not Dataplex problem, but data, data discovery problem.\n",
            "So, I'll ask you all, who here thinks that data discovery is easy? \n",
            "\n",
            "I don't see a single hand raised, and thank you for validating us. \n",
            "It's not easy, and my manager's in here too, it's not easy.\n",
            "\n",
            "Alright, so with that, I'm going to hand it over to Steve from Orange and he's going to tell us how they've enabled data discovery with GCP. \n",
            "\n",
            "Thanks Cynthia, and we're on a very, very similar journey.\n",
            "\n",
            "So, Orange, we're one of the largest telecoms providers in the world. \n",
            "We also sell a lot of IT services across 26 countries. \n",
            "So, we were originally France Telecom, and then we acquired operations in many other countries ranging from Belgium and Spain and Poland as well as, uh, African countries ranging from Senegal and Ivory Coast to the Democratic Republic of Congo.\n",
            "So, we have an enormously diverse set of challenges with almost 300 million customers across those countries.\n",
            "And the company's really a very proud technological company.\n",
            "A lot of the reasons why you have power saving mode in 5G today is because Orange cared about the impact, uh, of these technologies on the environment um for, for decades.\n",
            "And we're also at the forefront of a lot of AI work.\n",
            "I'm, I'm really lucky to have a dedicated AI research team that came from France Telecom Labs.\n",
            "And so, in my central team we have not only all the data engineering, data science, and ML engineering, but also, as I mentioned, the pure research team. \n",
            "And, we're really focused in three domains, uh, and we use super powering interchangeably with AI.\n",
            "So, we say that we're trying to superpower our employees' daily lives.\n",
            "We're trying to superpower all of our networks and we're trying to superpower our customer experiences. \n",
            "And, as Cynthia was describing, there's many challenges in providing these kinds of services at scale.\n",
            "\n",
            "So, before we started to use Google Cloud, we had data in organizational silos that were mapped to the physical infrastructure. \n",
            "So, each of these teams within the countries, like the network team, the finance team, they had built and maintained their own data infrastructure that led to these silos that map to these cultu- cultural and and, uh, and operational silos that we faced. \n",
            "And, across these 26 countries, the, the data infrastructure that we had was incredibly heterogeneous, and mostly had been um self-integrated, uh, and, and managed.\n",
            "And so, the, the level of complexity of maintaining that infrastructure, and the skills necessary for the teams to manage that infrastructure were extremely complex.\n",
            "And, and that a lot of the time that was taken by the data engineering teams was just manage, keeping the lights on on our infrastructure.\n",
            "\n",
            "And data governance was also managed, uh, very, very manually, uh, with these very basic systems and we had many different security and regulatory risk that we had to mitigate um through these systems.\n",
            "And as Cynthia was saying, a lot of our executives didn't really see data governance as strategic.\n",
            "Uh, they saw it as something that was just a, a, a, like a regulatory compliance requirement.\n",
            "They didn't see it as enabling, uh, the, our ability to reach AI at scale. \n",
            "And so, that was really preventing us from taking advantage of these enormous volumes of data that we generate across the, the business to generate value from that.\n",
            "\n",
            "So, we established a few years ago now, this vision of a data democracy where we make this data widely available within each country by breaking these silos that we have between the organizations. \n",
            "By having very rich data discovery and to do this at scale we use policy as code to not only enforce access control, but also things um like all of the data processes that we have for maintaining quality through the pipeline.\n",
            "And that allowed us to really use standard CICD techniques and tooling to dramatically improve the way that we manage our data.\n",
            "And also using AI itself to identify anomalies in the pipelines, uh, has been very, very useful. \n",
            "\n",
            "And now, our CEOs, also because of the tsunami of AI, they're really seeing data itself as being really foundational and crucial to the business.\n",
            "And, the thing that we did to encourage that was we set two things.\n",
            "One was, we set a, a uniform way to measure value on use cases across all 26 countries and that's widely available on a dashboard that every CEO and employee can see.\n",
            "So, they can see which use cases are generating a lot of value.\n",
            "But also, we have other operational KPIs that relate to our data migrations and data quality that are also public.\n",
            "And so, what that did was it created a competitive dynamic between our CEOs, which was really effective.\n",
            "And it also led for individual people in the company to see which countries were being really successful at different parts of their journey towards AI at scale and to encourage them to learn from one another.\n",
            "\n",
            "And so, we took inspiration from data mesh to then build a set of data products.\n",
            "But our approach to data products is that we have a centralized team, my team, that defines architecture with partners like Google and that allows us to have uniform infrastructure that's provided to each of these teams that's generating data, and then they're responsible for maintaining the freshness and the documentation and the quality of that data. \n",
            "And, and, and, and the level of automation that we're providing, uh, enables this really rich, um, set of outputs and and value that's getting generated by the use cases in these countries. \n",
            "\n",
            "So, this is what it looks like.\n",
            "There's really three pillars. \n",
            "The first is the data products that relate to data management and data quality, and also role-based, um, access with policy as code. \n",
            "So, one of the things that's really powerful, uh, with Dataplex and BigQuery in partnership, is that you can have very clear role-based access to data, but also on a column level, uh, which is really powerful because there's certain users that we have that we don't want them to have be able to see the really sensitive data, but we want to enable them to use that data for data operations.\n",
            "\n",
            "The second part is the self-service platform that we built based on GitLab and leverages this great interaction between BigQuery, Dataplex and Vertex. \n",
            "So, for me it's this golden triangle of the ability to use, the best, we think the best data infrastructure in the world on BigQuery, with Vertex where we get not only the best of the Google first-party models and tools, but also through the Model Garden we get, uh, leading, state-of-the-art, open-weight models, as well as state-of-the-art, open-source tooling for managing the model lifecycle.\n",
            "And, I've never seen a pace of innovation in my entire career than what we see in open-source tooling, as well as open-source and open-weight LLMs.\n",
            "\n",
            "So, being able to manage that as policy as code between all of the business decision makers, whether they be the, the, the, the data governors, our engineers, and the business owners has allowed us to really operate this at a much higher scale than was possible before.\n",
            "\n",
            "And then lastly, we federate all of that with governance to harmonize not only access to the data and the documentation, but also the catalogs and the forms of discovery. \n",
            "\n",
            "So, let me talk next about what we want to do next.\n",
            "\n",
            "We've built this early implementation of data mesh on top of the Google platform, and what we've seen is that the Dataplex tool is incredibly useful for us for things like data catalog, auto DQ, data loss prevention.\n",
            "\n",
            "And the other thing that's been great about working with the Dataplex team, in addition to the fact that they've been extremely reactive to understanding our challenges and providing us, uh, great interaction with the, the, the roadmap and influencing the engineering team, but also, they've been really good about providing open APIs to our other partners to allow them to have rich synchronization between their tooling environment and Dataplex itself.\n",
            "So, for example, uh, in the case of Collibra, we use Collibra today across the company to manage data that's not on GCP, that's on a lot of existing, um, infrastructure, um, that hasn't been migrated yet. \n",
            "So, having the ability to have really rich interaction between our existing data governance infrastructure and what we're building in Dataplex has been extremely powerful.\n",
            "\n",
            "So, where we want to go, is we have this vision of using AI itself, um, to have a marketplace for both the data that's within BigQuery, but also within Vertex.\n",
            "And so, the idea that we can use natural language as a way for anyone in the company within this marketplace to query what data is available, to have very, very quick business intelligence visualizations of that data, and be able to answer really direct, simple questions and have a dialogue with the data, even before they engage, uh, a data scientist or data engineer, uh, really unlocks an enormous amount of value in the company. \n",
            "And, and we think that it's a fundamental shift in the technological interaction with computers, where you can use natural language as if at, at, at the, at the level of power that previously you needed to be a programmer to achieve. \n",
            "\n",
            "And, and then lastly, using AI to detect anomalies in our pipelines, uh, to help us fill, uh, where we have gaps in our data, and otherwise to make sure that the data that we're generating is of high quality because it's clear that without extremely high quality data, we're not going to have high quality outputs from our AI systems. \n",
            "And then, that applies not only to systems where we're just doing inference on existing large language models, but it's also very true when we try to fine-tune models, so then, for them to be much smaller, uh, and to, and to operate, uh, much faster. \n",
            "So, again, having extremely high-quality data, where we're managing the lineage of that data, and that's really easily accessible to the, the teams that are working on the AI fine-tuning, uh, has been really transformative. \n",
            "And so, this data democracy for us, is all about having this data easily accessible, in extremely high quality, that's well documented, including by having generative AI generate, uh, gaps in documentation and identify, uh, uh, missing elements um and having that integrated extremely well into the workflow of our employees. \n",
            "And we think that this data democracy will unlock, unlock an enormous amount of value across the company, because the, the amount of data that we're generating today that's been very hard to manage in the past, now with this more uniform infrastructure that's not only available for us on public cloud, but also, we've been working very closely with Google over the last few years to have on-premise data infrastructure, which we announced today.\n",
            "So, we have a GDC edge, uh, uh, infrastructure that we can deploy in our own data centers in each country that also has, uh, data management and AI capability. \n",
            "So, it gives us this really rich environment, between hybrid, between on-prem and public cloud because we have to respond not only to very varying regulatory requirements across our countries that change often unpredictably, um, but also we have commercial constraints because the amount of data that's coming off our network is enormous.\n",
            "So, for example, just the network telemetry data which is the data we use to operate the network is over a petabyte a day. \n",
            "So, having something sophisticated on-premise to allow us to filter that data before we send to public cloud and to do that in a way that maintains quality and this policy of code, uh, mechanism is, is extremely transformative.\n",
            "\n",
            "So, let me link bring Lou back up to talk about what's on the roadmap.\n",
            "\n",
            "Alright, thank you so much Steve and Cynthia for sharing your use cases and perspectives. \n",
            "Those are really, really wonderful insights.\n",
            "\n",
            "And I think we can all resonate with just how critical it is to have this platform with self-serve discovery and well-governed data.\n",
            "It's not easy, but that's what we're here for. \n",
            "\n",
            "So, next, let's take a look at what are the new launches we're very excited to announce this time.\n",
            "\n",
            "First and foremost, everything in Dataplex starts from having this unified metadata across distributed data. \n",
            "And that's exactly where automatic cataloging comes in. \n",
            "\n",
            "We have worked very closely with various GCP services and products in order to ingest that metadata, to harvest metadata, and index metadata for search. \n",
            "And based on this, you will be able to discover your assets across analytics, data lakes, databases, AI and BI services. \n",
            "You are also able to enrich and organize this data to track lineage, to enforce governance policies and really having the solid foundation for data-to-AI governance.\n",
            "\n",
            "Already, Dataplex supports a rich set of data sources, such as BigQuery and Pub/Sub.\n",
            "And today, we're super excited to announce a host of new integrations as you can see here.\n",
            "\n",
            "First are the Vertex AI related launches.\n",
            "We're very excited to be announcing the GA of automated cataloging for Vertex AI models and datasets, and also the preview of automated cataloging for Vertex AI features.\n",
            "\n",
            "With those integrations in place, as soon as you create a new artifact in Vertex AI, they will be made searchable in Dataplex in near real time. \n",
            "And this is really critical, because we truly believe that data and AI should be managed and governed in a consistent and coherent way.\n",
            "\n",
            "Next, are operational databases, including the GA of Bigtable integration, Spanner integration, as well as the preview of automated metadata cataloging from Cloud SQL. \n",
            "And it's super important to have this coverage for operational databases as well, to really provide end-to-end visibility. \n",
            "\n",
            "Next, we're also actively working on Looker integration and it's a launch that's coming soon, so please stay tuned.\n",
            "\n",
            "And with all of those launches in place, our goal is to really provide a powerful metadata foundation to you to enable automated metadata discovery, management, and governance. \n",
            "\n",
            "Next, is lineage.\n",
            "\n",
            "So, Dataplex already provides the ability for you to automatically track and visualize lineage as your data artifacts flow through your distributed data landscape.\n",
            "Now, this capability also works nicely with other Dataplex features, such as data quality checks.\n",
            "Where as soon as a data quality issue is discovered, you would be able to trace upstream and downstream in order to understand what is the root cause and impact of a particular data quality breach.\n",
            "\n",
            "Now, with lineage parsing, there's already native integration with services like BigQuery, DataProc, and Composer. \n",
            "And we also have Dataplex API and open lineage integration to really provide that extensibility.\n",
            "\n",
            "And today, we're really excited to announce the lineage support for Vertex AI pipelines. \n",
            "Really allowing for this end-to-end traceability, from data processing to data analytics to machine learning training and deployment. \n",
            "And providing you with this end-to-end picture that is critical for data-to-AI governance and compliance.\n",
            "\n",
            "Now, at the same time, in addition to extending the type of data sources being covered, we're also enhancing the granularity of lineage tracking.\n",
            "We're very excited to introduce the preview for column-level lineage in BigQuery.\n",
            "\n",
            "Oh! Hey! Thank you! \n",
            "\n",
            "So, ever since introducing table lineage in BigQuery, as well as other services, we have seen strong customer enthusiasm adoption thanks to all of you. \n",
            "And we're also getting very strong demand for the next level granularity which is what we're very excited to bring to you today.\n",
            "So, now you're able to perform root cause analysis and impact analysis at the column level in addition to at the table level. \n",
            "And imagine when you have a column that's identified to contain personal identifiable information, this is where column-level lineage really shines, right? Where you're able to then control its propagation and then be able to comply with different regulations.\n",
            "\n",
            "Now, there is also more ease-of-use features that we're launching together with this. \n",
            "So, for example, there's the ability to help you pull up all the upstreams and all the downstreams of a particular node in the lineage graph. \n",
            "There's also the ability to filter by different transformation types to make lineage graph more consumable.\n",
            "And there's also the ability to export lineage for offline analysis. \n",
            "So, all of this is to enhance our user experience and to make it easier to work with Dataplex lineage. \n",
            "\n",
            "Next are two Gen AI powered Gemini launches from Dataplex.\n",
            "\n",
            "So, first of all, we know that searching over metadata is a really critical experience with Dataplex, and it's really at the core of what we do here at Dataplex.\n",
            "Now, in addition to doing keyword search with Dataplex, you're able to just ask us the question in natural language and Dataplex will be able to interpret your intent and be able to retrieve the most relevant search results.\n",
            "\n",
            "This can really go a long way to lower this entry barrier as we have discussed earlier, and to really democratize experience of data discovery to your entire organization. \n",
            "\n",
            "Now, once the data is discovered, there's another really exciting Gen AI powered features from Dataplex to help which is data insights.\n",
            "\n",
            "Now, a lot of us working with data must have experienced the cold-start problem. \n",
            "Now, which is once you find a valuable data asset, you're sometimes not sure what is the best SQL queries to write in order to really extract that meaningful insight from the data. \n",
            "So, that's exactly where data insight is here to help. \n",
            "It would automatically generate and suggest SQL queries as well as a list of questions you can ask of a table in natural language. \n",
            "And it will provide validated SQL queries to you as well that is ready to run in BigQuery Studio.\n",
            "\n",
            "So, this could really help give you a jump start into your analysis journey and to really help accelerate time to insight for all of us. \n",
            "\n",
            "Next is data governance, our favorite topic. \n",
            "\n",
            "So, as we know, metadata is the core of everything we do here at Dataplex, right?\n",
            "So, we're constantly thinking, in addition to help you better discover and better understand this data, can we also make metadata more actionable to help you drive active actions in terms of data governance operations?\n",
            "\n",
            "So, this is exactly the motivation for governance rules.\n",
            "Where we start from the metadata you already have in Dataplex, whether it's technical metadata or business metadata.\n",
            "And then, you will be able to define and enforce governance policies at scale with the help of Dataplex. \n",
            "\n",
            "So, here's how it works. \n",
            "First of all, you start by writing a search query in Dataplex to identify all the entries and fields that are relevant for a particular governance policy to be applied. \n",
            "And then you can define your policy in the form of governance rules with the help of Dat"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting\n",
        "from webvtt import WebVTT\n",
        "\n",
        "generation_config = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "}\n",
        "\n",
        "def generate_transcript(audio_file, vtt_file):\n",
        "    vertexai.init(project=\"hyunuk-ai-dev\", location=\"us-central1\")\n",
        "    SI = \"\"\"## TASK: Generate Natural Transcript\n",
        "\n",
        "        I have an audio file and its corresponding transcript in VTT format.\n",
        "        I want you to generate a natural-souding transcript of the audio, improving upon\n",
        "        the provided VTT captions, and then translate the improved transcript.\n",
        "\n",
        "        ### Instructions:\n",
        "        1. **Listen** to the audio and use the VTT transcript as a starting point.\n",
        "        2. **Improve** the transcript by making it sound more natural and conversational.\n",
        "           This might involve:\n",
        "           - Correcting any errors in the VTT captions.\n",
        "           - Removing unnecessary pauses or filler words.\n",
        "           - Rephrasing sentences to improve clarity and flow.\n",
        "        3. **Output** the final translation, sentence by sentence.\n",
        "        \"\"\"\n",
        "\n",
        "    model = GenerativeModel(\n",
        "        \"gemini-pro-experimental\",\n",
        "        system_instruction=[SI],\n",
        "        )\n",
        "\n",
        "    audio = Part.from_uri(\n",
        "        mime_type=\"audio/mpeg\",\n",
        "        uri=\"gs://visual_learner/3A855rN_9pE.mp3\",\n",
        "    )\n",
        "\n",
        "    text = Part.from_text(\n",
        "        text=vtt_file\n",
        "    )\n",
        "\n",
        "    safety_settings = [\n",
        "        SafetySetting(\n",
        "            category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    responses = model.generate_content(\n",
        "        [audio, text],\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        stream=True,\n",
        "    )\n",
        "    for response in responses:\n",
        "        print(response.text, end=\"\")\n",
        "\n",
        "audio_file = \"3A855rN_9pE.mp3\"\n",
        "vtt_file = \"3A855rN_9pE.en.vtt\"\n",
        "generate_transcript(audio_file, vtt_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QI-T-02LEOGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}